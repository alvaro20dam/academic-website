{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c05f610-7c2d-45bb-bc14-44828ba80ba2",
   "metadata": {},
   "source": [
    "# Regularized Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836974f-c750-45cf-bd85-4ae9095fbf3c",
   "metadata": {},
   "source": [
    "**Regularization in Linear Regression**\r\n",
    "\r\n",
    "Regularization is a technique used to prevent overfitting in linear regression models. Overfitting occurs when a model performs well on the training data but poorly on unseen data. This happens because the model has learned the noise in the training data, rather than the underlying patterns.\r\n",
    "\r\n",
    "**How Regularization Works**\r\n",
    "\r\n",
    "Regularization techniques add a penalty term to the loss function. This penalty term discourages the model from assigning large weights to the features. As a result, the model becomes simpler and less prone to overfitting.\r\n",
    "\r\n",
    "**Common Regularization Techniques**\r\n",
    "\r\n",
    "1. **Ridge Regression (L2 Regularization)**\r\n",
    "\r\n",
    "   - Adds the squared sum of the weights to the loss function.\r\n",
    "   - Shrinks the weights towards zero but does not force any weights to be exactly zero.\r\n",
    "   - Effective when many features have a small but non-zero effect on the target variable.\r\n",
    "\r\n",
    "2. **Lasso Regression (L1 Regularization)**\r\n",
    "\r\n",
    "   - Adds the absolute sum of the weights to the loss function.\r\n",
    "   - Can force some weights to be exactly zero, effectively performing feature selection.\r\n",
    "   - Useful when you believe only a few features are truly important.\r\n",
    "\r\n",
    "3. **Elastic Net Regression**\r\n",
    "\r\n",
    "   - Combines L1 and L2 regularization.\r\n",
    "   - Balances the benefits of both Ridge and Lasso regression.\r\n",
    "   - Can be useful when you have a mix of many weakly important features and a few strongly important ones.\r\n",
    "\r\n",
    "**Choosing the Right Regularization Technique**\r\n",
    "\r\n",
    "The choice of regularization technique depends on the specific problem and the characteristics of the data.\r\n",
    "\r\n",
    "- **Ridge Regression** is often a good starting point, as it generally improves stability and reduces overfitting.\r\n",
    "- **Lasso Regression** can be useful for feature selection and when you believe many features are irrelevant.\r\n",
    "- **Elastic Net** provides a balance between Ridge and Lasso and can\n",
    "[Image of Ridge and Lasso Regression]\r\n",
    "\r\n",
    "**In Summary**\r\n",
    "\r\n",
    "Regularization is a powerful technique for improving the performance of linear regression models, especially when dealing with high-dimensional data or when overfitting is a concern. By carefully selecting the appropriate regularization technique and tuning the hyperparameters, you can achieve better generalization and more robust models.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24402597-c5b5-4210-a491-275efb38968e",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a9e942-8801-4ce5-a592-161af0357ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81113ad-6736-49fa-a076-7232403cbd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b1b26a-7a2e-46a7-8b15-c940758c2d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.00675066]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f91edf-6504-40b7-a0a3-10846d80be5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.98943496])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f442797-f2c7-400c-aa38-03978f2ccc9b",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7157d71-08a7-47e5-8ab1-07e3b4a676dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
